{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실행X : 전처리)전체 API scanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path0_dir = './0'\n",
    "path1_dir = './1'\n",
    "file0_list = os.listdir(path0_dir)\n",
    "file1_list = os.listdir(path1_dir)\n",
    "total_API=[]\n",
    "for i in file0_list:\n",
    "    file = './0/%s' % i\n",
    "    data = open(file,'r')\n",
    "    API_list = data.readlines()\n",
    "    API_list = list(set(API_list))\n",
    "    for i in range(len(API_list)):\n",
    "        API_list[i] = (API_list[i])[:-1]\n",
    "    total_API.extend(API_list)\n",
    "    total_API = list(set(total_API))\n",
    "for i in file1_list:\n",
    "    file = './1/%s' % i\n",
    "    data = open(file,'r')\n",
    "    API_list = data.readlines()\n",
    "    API_list = list(set(API_list))\n",
    "    for i in range(len(API_list)):\n",
    "        API_list[i] = (API_list[i])[:-1]\n",
    "    total_API.extend(API_list)\n",
    "    total_API = list(set(total_API))\n",
    "\n",
    "num_of_total_API = np.size(total_API)\n",
    "total_API = sorted(total_API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (실행X : 전처리) train data와 test data 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data0 = np.zeros((1,dim))\n",
    "train_data1 = np.zeros((1,dim))\n",
    "test_data0 = np.zeros((1,dim))\n",
    "test_data1 = np.zeros((1,dim))\n",
    "\n",
    "count=0;\n",
    "for i in file0_list:\n",
    "    input_data= np.zeros((1,dim))\n",
    "    if(count >= len(file0_list)*(0.7) ):\n",
    "        file = './0/%s' % i\n",
    "        data= open(file,'r')\n",
    "        sample = data.readlines()\n",
    "        for j in range(len(sample)):\n",
    "            sample[j] = (sample[j])[:-1]\n",
    "        sample = sorted(sample)\n",
    "        for j in range(len(sample)):\n",
    "            for k in range(len(total_API)):\n",
    "                if(sample[j] == total_API[k]):\n",
    "                    input_data[0][k] +=1\n",
    "                    break;\n",
    "        if(count==len(file0_list)*(0.7)):\n",
    "            test_data0 = input_data;\n",
    "        else:\n",
    "            test_data0 = np.concatenate((test_data0,input_data),axis=0)\n",
    "    else:\n",
    "        file = './0/%s' % i\n",
    "        data = open(file,'r')\n",
    "        sample = data.readlines()\n",
    "        for j in range(len(sample)):\n",
    "            sample[j] = (sample[j])[:-1]\n",
    "        sample = sorted(sample)\n",
    "        for j in range(len(sample)):\n",
    "            for k in range(len(total_API)):\n",
    "                if(sample[j] == total_API[k]):\n",
    "                    input_data[0][k] +=1\n",
    "                    break;\n",
    "        if(count==0):\n",
    "            train_data0 = input_data;\n",
    "        else:\n",
    "            train_data0 = np.concatenate((train_data0,input_data),axis=0)\n",
    "        \n",
    "    count +=1\n",
    "        \n",
    "count=0;\n",
    "for i in file1_list:\n",
    "    input_data= np.zeros((1,dim))\n",
    "    if(count >= len(file1_list)*(0.7) ):\n",
    "        file = './1/%s' % i\n",
    "        data= open(file,'r')\n",
    "        sample = data.readlines()\n",
    "        for j in range(len(sample)):\n",
    "            sample[j] = (sample[j])[:-1]\n",
    "        sample = sorted(sample)\n",
    "        for j in range(len(sample)):\n",
    "            for k in range(len(total_API)):\n",
    "                if(sample[j] == total_API[k]):\n",
    "                    input_data[0][k] +=1\n",
    "                    break;\n",
    "        if(count==len(file1_list)*(0.7)):\n",
    "            test_data1 = input_data;\n",
    "        else:\n",
    "            test_data1 = np.concatenate((test_data1,input_data),axis=0)\n",
    "    else:\n",
    "        file = './1/%s' % i\n",
    "        data = open(file,'r')\n",
    "        sample = data.readlines()\n",
    "        for j in range(len(sample)):\n",
    "            sample[j] = (sample[j])[:-1]\n",
    "        sample = sorted(sample)\n",
    "        for j in range(len(sample)):\n",
    "            for k in range(len(total_API)):\n",
    "                if(sample[j] == total_API[k]):\n",
    "                    input_data[0][k] +=1\n",
    "                    break;\n",
    "        if(count==0):\n",
    "            train_data1 = input_data;\n",
    "        else:\n",
    "            train_data1 = np.concatenate((train_data1,input_data),axis=0)\n",
    "            \n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 API 목록 및 개수, train data와 test data 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "opSet = np.load('./opSet.npz')\n",
    "num_of_total_API = opSet['num_of_total_API']\n",
    "total_API= opSet['total_API']\n",
    "\n",
    "array = np.load(\"./dataset.npz\")\n",
    "train_data0 = array['train_data0']\n",
    "train_data1 = array['train_data1']\n",
    "test_data0 = array['test_data0']\n",
    "test_data1 = array['test_data1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data에서 TF 및 TF_IDF값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0TF = np.zeros(np.shape(train_data0))\n",
    "train1TF = np.zeros(np.shape(train_data1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF = log( 1 + w/d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(np.size(train0TF,0)):\n",
    "    for j in range(np.size(train0TF,1)):\n",
    "        train0TF[i][j] = np.log(1+ (train_data0[i][j]/num_of_total_API))\n",
    "for i in range( np.size(train1TF,0) ) :\n",
    "    for j in range(np.size(train1TF,1)):\n",
    "        train1TF[i][j] = np.log(1+ (train_data1[i][j]/num_of_total_API))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF = 1\n",
    "#### TF_IDF = TF * IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0TF_IDF = train0TF * 1 \n",
    "train1TF_IDF = train1TF * 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 API에 대한 평균 TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0AVG_TF_IDF = sum(train0TF_IDF)/ np.size(train0TF_IDF,0)\n",
    "train1AVG_TF_IDF = sum(train1TF_IDF) / np.size(train1TF_IDF,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data에 대한 악성코드 분류\n",
    "#### by cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "answer0_num=0;\n",
    "answer1_num=0;\n",
    "alpha = 0.0000000001 ## zero devide error를 막기위한 값\n",
    "for i in range(np.size(train0TF_IDF,0)):\n",
    "    data = train0TF_IDF[i]\n",
    "    similariry0 = np.dot(data,train0AVG_TF_IDF)/np.sqrt(np.dot(data,data) * np.dot(train0AVG_TF_IDF,train0AVG_TF_IDF)+ alpha)\n",
    "    similariry1 = np.dot(data,train1AVG_TF_IDF)/np.sqrt(np.dot(data,data) * np.dot(train1AVG_TF_IDF,train1AVG_TF_IDF)+ alpha)\n",
    "    if(similariry0 >= similariry1):\n",
    "        answer0_num +=1\n",
    "        \n",
    "for i in range(np.size(train1TF_IDF,0)):\n",
    "    data = train1TF_IDF[i]\n",
    "    similariry0 = np.dot(data,train0AVG_TF_IDF)/np.sqrt(np.dot(data,data) * np.dot(train0AVG_TF_IDF,train0AVG_TF_IDF)+ alpha)\n",
    "    similariry1 = np.dot(data,train1AVG_TF_IDF)/np.sqrt(np.dot(data,data) * np.dot(train1AVG_TF_IDF,train1AVG_TF_IDF)+ alpha)\n",
    "    if(similariry0 <= similariry1):\n",
    "        answer1_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.62%\n",
      "53.69%\n"
     ]
    }
   ],
   "source": [
    "print('%0.2f' %float(answer0_num/np.size(train_data0,0) * 100) + '%')\n",
    "print('%0.2f' %float(answer1_num/np.size(train_data1,0) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data에서 TF_IDF값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0TF = np.zeros(np.shape(test_data0))\n",
    "test1TF = np.zeros(np.shape(test_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(np.size(test_data0,0)):\n",
    "    for j in range(np.size(test_data0,1)):\n",
    "        test0TF[i][j] = np.log(1+ (test_data0[i][j]/num_of_total_API))\n",
    "for i in range(np.size(test_data1,0)):\n",
    "    for j in range(np.size(test_data1,1)):\n",
    "        test1TF[i][j] = np.log(1+ (test_data1[i][j]/num_of_total_API))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0TF_IDF = test0TF\n",
    "test1TF_IDF = test1TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data에 대한 악성코드 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer0_num=0;\n",
    "answer1_num=0;\n",
    "test0_y = np.zeros(np.size(test0TF_IDF))\n",
    "test1_y = np.zeros(np.size(test1TF_IDF))\n",
    "for i in range(np.size(test0TF_IDF,0)):\n",
    "    data = test0TF_IDF[i]\n",
    "    similariry0 = np.dot(data,train0AVG_TF_IDF)/np.sqrt(  np.dot(data,data) * np.dot(train0AVG_TF_IDF,train0AVG_TF_IDF)+alpha )\n",
    "    similariry1 = np.dot(data,train1AVG_TF_IDF)/np.sqrt(   np.dot(data,data) * np.dot(train1AVG_TF_IDF,train1AVG_TF_IDF)+alpha )\n",
    "    if(similariry0 >= similariry1):\n",
    "        answer0_num +=1\n",
    "        test0_y[i]=0\n",
    "    else:\n",
    "        test0_y[i]=1\n",
    "        \n",
    "for i in range(np.size(test1TF_IDF,0)):\n",
    "    data = test1TF_IDF[i]\n",
    "    similariry0 = np.dot(data,train0AVG_TF_IDF)/np.sqrt(  np.dot(data,data) * np.dot(train0AVG_TF_IDF,train0AVG_TF_IDF)+alpha )\n",
    "    similariry1 = np.dot(data,train1AVG_TF_IDF)/np.sqrt(   np.dot(data,data) * np.dot(train1AVG_TF_IDF,train1AVG_TF_IDF)+alpha )\n",
    "    if(similariry0 <= similariry1):\n",
    "        answer1_num +=1\n",
    "        test1_y[i] =1\n",
    "    else:\n",
    "        test1_y[i] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83.30%\n",
      "54.17%\n"
     ]
    }
   ],
   "source": [
    "print('%0.2f' %float(answer0_num /np.size(test0TF_IDF,0) * 100) + '%')\n",
    "print('%0.2f' %float(answer1_num /np.size(test1TF_IDF,0) * 100) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
